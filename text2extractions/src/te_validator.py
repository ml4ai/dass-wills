import os
import json
import pandas as pd

def read_directory_files(directory, extension):
    """Read all files with a given extension from a directory."""
    files = {}
    for filename in os.listdir(directory):
        if filename.endswith(extension):
            file_path = os.path.join(directory, filename)
            base_name = os.path.splitext(filename)[0]  # Extract the base name without extension
            with open(file_path, 'r') as file:
                files[base_name] = file.read()
    return files

def extract_and_evaluate(json_obj, original_text, exclude_keys={"id", "events"}):
    """Evaluate JSON data against original text while maintaining the original structure."""
    match_count = 0
    mismatch_count = 0

    if isinstance(json_obj, dict):
        for key, value in json_obj.items():
            if key not in exclude_keys:
                evaluated_value, sub_match, sub_mismatch = extract_and_evaluate(value, original_text, exclude_keys)
                json_obj[key] = evaluated_value
                match_count += sub_match
                mismatch_count += sub_mismatch
        return json_obj, match_count, mismatch_count
    elif isinstance(json_obj, list):
        for i in range(len(json_obj)):
            json_obj[i], sub_match, sub_mismatch = extract_and_evaluate(json_obj[i], original_text, exclude_keys)
            match_count += sub_match
            mismatch_count += sub_mismatch
        return json_obj, match_count, mismatch_count
    else:
        # Exclude None values and boolean types from evaluation
        if json_obj is not None and not isinstance(json_obj, bool):
            value_str = str(json_obj).lower()
            start_offset = original_text.find(value_str)
            if start_offset != -1:
                end_offset = start_offset + len(value_str)
                return (
                    {
                        "value": json_obj,
                        "char_offsets": [start_offset, end_offset]
                    }, match_count + 1, mismatch_count
                )
            else:
                return (
                    {
                        "value": json_obj,
                        "char_offsets": "No match found. This may be a hallucination generated by the LLM."
                    }, match_count, mismatch_count + 1
                )
        return json_obj, match_count, mismatch_count  # Return the original value for None and boolean types

def run_evaluation(te_directory, text_directory, output_directory, report_file):
    """Run the evaluation for corresponding TE and text files, modify JSON, and generate a summary report."""
    te_files = read_directory_files(te_directory, '.json')
    text_files = read_directory_files(text_directory, '.txt')

    report_data = []
    total_matches = 0
    total_mismatches = 0

    for base_name, te_content in te_files.items():
        if base_name in text_files:
            original_text = text_files[base_name].lower()  # Convert text to lowercase for comparison

            # Parse the JSON content from the TE output
            try:
                te_json = json.loads(te_content)
            except json.JSONDecodeError:
                print(f"Error parsing JSON for {base_name}.json")
                continue

            # Run the evaluation and count matches and mismatches
            updated_data, match_count, mismatch_count = extract_and_evaluate(te_json, original_text)
            total_matches += match_count
            total_mismatches += mismatch_count

            # Save the updated JSON to the output directory
            output_path = os.path.join(output_directory, f"{base_name}_evaluated.json")
            with open(output_path, 'w') as output_file:
                json.dump(updated_data, output_file, indent=4)

            # Append data to the report
            report_data.append({
                "Filename": base_name,
                "Matches": match_count,
                "Mismatches": mismatch_count,
                "Total": match_count + mismatch_count
            })

            print(f"Evaluation completed for {base_name}: {match_count} matches, {mismatch_count} mismatches")

    # Append a total row to the report
    report_data.append({
        "Filename": "TOTAL",
        "Matches": total_matches,
        "Mismatches": total_mismatches,
        "Total": total_matches + total_mismatches
    })

    # Create a DataFrame and save to CSV
    df = pd.DataFrame(report_data)
    df.to_csv(report_file, index=False)
    print(f"Summary report saved to {report_file}")

# Main function to run the script
if __name__ == "__main__":
    te_directory = "/Users/alicekwak/repos/dass-wills/text2extractions/output/full_text_ID"  # Replace with your TE output directory path
    text_directory = "/Users/alicekwak/repos/dass-wills/text2extractions/input/full_text_ID"  # Replace with your original text directory path
    output_directory = "/Users/alicekwak/repos/dass-wills/text2extractions/evaluation_ID"  # Replace with your desired output directory path
    report_file = os.path.join(output_directory, "evaluation_summary.csv")  # Path for the CSV report

    # Ensure output directory exists
    os.makedirs(output_directory, exist_ok=True)

    # Run the evaluation for all corresponding files and generate the report
    run_evaluation(te_directory, text_directory, output_directory, report_file)
